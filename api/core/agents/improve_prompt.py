from typing import Any

import workflowai
from pydantic import BaseModel, Field

from core.domain.url_content import URLContent

from .chat_task_schema_generation.apply_field_updates import InputFieldUpdate, OutputFieldUpdate


class ImprovePromptAgentInput(BaseModel):
    class AgentConfig(BaseModel):
        prompt: str | None = Field(
            default=None,
            description="The original agent prompt to improve",
        )

        input_schema: dict[str, Any] | None = Field(
            default=None,
            description="The original input schema of the agent",
        )
        output_schema: dict[str, Any] | None = Field(
            default=None,
            description="The original output schema of the agent",
        )

    original_agent_config: AgentConfig | None = Field(
        default=None,
        description="The original agent config, to improve",
    )

    class AgentRun(BaseModel):
        run_input: str | None = Field(
            default=None,
            description="A sample input that, based on the 'original_agent_config.prompt', has yielded the 'run_output' as output",
        )

        run_output: str | None = Field(
            default=None,
            description="The output that was generated by the agent based on the 'run_input' and the 'original_agent_config.prompt'",
        )
        user_evaluation: str | None = Field(
            default=None,
            description="The user's evaluation of the 'run_output'",
        )
        user_evaluation_url_contents: list[URLContent] | None = Field(
            default=None,
            description="URL content extracted from the 'user_evaluation', if the 'user_evaluation' contains URLs",
        )

    agent_run: AgentRun | None = Field(
        default=None,
        description="A user-evaluated run, obtained using the 'original_agent_config', to be used as the basis for improving the agent, if absent this means the 'user_evaluation' is about the overall performance of the agent",
    )

    user_evaluation: str | None = Field(
        default=None,
        description="The user's evaluation of the agent's outputs",
    )

    available_tools_description: str | None = Field(
        default=None,
        description="The description of the available tools that the agent can use, in order to generate an output'",
    )


class ImprovePromptAgentOutput(BaseModel):
    improved_prompt: str | None = Field(
        default=None,
        description="The improved agent prompt",
    )
    changelog: list[str] | None = Field(
        default=None,
        description="The changelog of the improvements between the orginal 'prompt' and the 'improved_agent_prompt'",
    )
    input_field_updates: list[InputFieldUpdate] | None = Field(
        default=None,
        description="The list of input field updates to apply to the orignal 'input_schema', if any",
    )
    output_field_updates: list[OutputFieldUpdate] | None = Field(
        default=None,
        description="The list of output field updates to apply to the original 'output_schema', if any",
    )


@workflowai.agent(
    id="improve-prompt",
)
async def run_improve_prompt_agent(_: ImprovePromptAgentInput) -> ImprovePromptAgentOutput:
    """Given an original agent config (prompt, input schema, output schema), an optiona example agent run (input, output) and a user evaluation, generate the following:

    - An improved prompt
    - A changelog
    - A list of field updates for the input schema
    - A list of field updates for the output schema

    Analyze the inputs to create an 'improved_prompt' that addresses the issues mentioned in the user evaluation and aims to produce better results than the original prompt. The prompt must be kept generic; any eventual addition based on 'prompt_input' or 'prompt_output' must be clearly stated as 'example'. Consider the specific content of the 'prompt_input' when crafting the improved prompt to ensure it's tailored to the task at hand. Do NOT mention "JSON" anywhere in the 'improved_prompt'. Do NOT add markdown in 'improved_prompt'. IF, AND ONLY IF the user asks to add a tool capability in the 'user_evaluation', field you can add tools from 'available_tools_description' by using "@<the tool name>" in the 'improved_prompt'.

    Make sure to take into account the full content of the 'original_agent_config.prompt'. Do not omit anything.

    Additionally, generate a list of 'input_field_updates' for the output schema. Each field update should include:

    - 'keypath': The path to the field in the schema (e.g., 'team.name' or 'players.0.name' for arrays; make sure to always use '0' to update the item properties of an array)
    - 'updated_description': An improved description for the field, if necessary

    Additionally, generate a list of 'output_field_updates' for the output schema. Each field update should include:

    - 'keypath': The path to the field in the schema (e.g., 'team.name' or 'players.0.name' for arrays; make sure to always use '0' to update the item properties of an array)
    - 'updated_description': An improved description for the field, if necessary
    - 'updated_examples': New or modified examples for the field, if applicable. Strictly restrict field examples updates to string fields without format (no "html", etc.). IMPORTANT: do not add examples if the fields does not have existing examples. Only update examples the existing examples contradict with the 'user_evaluation'.

    Always make sure the 'keypath' in 'input_field_updates' and 'output_field_updates' exist.

    Only include field updates where changes to descriptions or examples will improve the schema's clarity or effectiveness. Ensure that any changes to the description or examples align with the improvements made to the prompt and address any relevant issues from the user evaluation.

    Provide a 'changelog' that lists the specific changes made to the original promp. Each item should briefly describe a modification or addition made to improve the prompt. This also includes the 'field_updates'"""
    ...

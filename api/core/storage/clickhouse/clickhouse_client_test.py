import asyncio
import datetime
import os
import time
from collections.abc import Collection
from pathlib import Path
from typing import Any
from urllib.parse import urlparse

import bson
import pytest

from core.domain.error_response import ErrorResponse
from core.domain.fields.internal_reasoning_steps import InternalReasoningStep
from core.domain.llm_completion import LLMCompletion
from core.domain.llm_usage import LLMUsage
from core.domain.models import Provider
from core.domain.search_query import (
    SearchField,
    SearchOperation,
    SearchOperationSingle,
    SearchOperator,
    SearchQuery,
    SearchQueryNested,
    SearchQuerySimple,
    SingleValueOperator,
)
from core.domain.task_run import Run
from core.domain.task_run_aggregate_per_day import TaskRunAggregatePerDay
from core.domain.task_run_query import SerializableTaskRunQuery
from core.domain.tool_call import ToolCall, ToolCallRequestWithID
from core.storage.clickhouse.clickhouse_client import (
    ClickhouseClient,
)
from core.storage.clickhouse.models.runs import ClickhouseRun
from core.storage.mongo.models.task_run_document import TaskRunDocument
from core.storage.task_run_storage import RunAggregate
from core.utils.fields import datetime_factory
from core.utils.schemas import FieldType
from core.utils.uuid import uuid7
from tests.models import task_group, task_run_ser
from tests.utils import fixture_bytes, fixtures_json


def read_sql_commands() -> list[str]:
    setup_sql_commands = (Path(__file__).parent / "migrations/m2025_02_10_init.sql").read_text().splitlines()
    lines_per_command: list[list[str]] = [[]]
    # Remove all lines that start with --
    for line in setup_sql_commands:
        line = line.strip()
        if not line or line.startswith("--"):
            continue
        lines_per_command[-1].append(line)
        if line.endswith(";"):
            lines_per_command.append([])

    return ["\n".join(lines) for lines in lines_per_command if lines]


async def fresh_clickhouse_client(dsn: str | None = None):
    from clickhouse_connect.driver.exceptions import DatabaseError

    if not dsn:
        dsn = os.getenv("CLICKHOUSE_TEST_CONNECTION_STRING", "clickhouse://default:admin@localhost:8123/db_test")

    if "localhost" not in dsn:
        raise ValueError("Only local testing is supported")
    client = ClickhouseClient(dsn, tenant_uid=1)

    db_name = urlparse(dsn).path.lstrip("/")

    try:
        await client.command("DROP TABLE IF EXISTS runs;")
    except DatabaseError as e:
        if f"Database {db_name} does not exist" not in str(e):
            raise e

        from clickhouse_connect.driver import create_async_client  # pyright: ignore[reportUnknownVariableType]

        raw_clt = await create_async_client(dsn=dsn.rstrip(f"/{db_name}"))
        await raw_clt.command(f"CREATE DATABASE {db_name};")  # pyright: ignore[reportUnknownMemberType]

    setup_commands = read_sql_commands()
    # Skipping settings for default user since it breaks in local dev

    assert setup_commands[0].startswith("CREATE TABLE runs"), "sanity check"
    await client.command(setup_commands[0])

    assert setup_commands[1].startswith(
        "ALTER TABLE runs ADD INDEX cache_hash_index cache_hash TYPE bloom_filter(0.01);",
    ), "sanity check"
    await client.command(setup_commands[1])

    return client


@pytest.fixture(scope="module")
async def clickhouse_client():
    return await fresh_clickhouse_client()


@pytest.fixture(scope="function", autouse=True)
async def truncate_run_table(clickhouse_client: ClickhouseClient):
    await clickhouse_client.command("TRUNCATE TABLE runs;")


def _uuid7(v: int):
    return uuid7(ms=lambda: 0, rand=lambda: v)


_TASK_TUPLE = ("task_1", 1)


async def test_insert_and_fetch_run(clickhouse_client: ClickhouseClient):
    run = task_run_ser(
        id=str(uuid7()),
        cost_usd=11,
        duration_seconds=12,
        task_uid=1,
        reasoning_steps=[InternalReasoningStep(title="test", explaination="test")],
        tool_calls=[ToolCall(id="test", tool_name="1", tool_input_dict={"test": "test"}, result="bla")],
        tool_call_requests=[ToolCallRequestWithID(id="test", tool_name="1", tool_input_dict={"test": "test"})],
        llm_completions=[
            LLMCompletion(
                messages=[{"bla": "bla"}],
                usage=LLMUsage(prompt_token_count=1, completion_token_count=1),
                provider=Provider.OPEN_AI,
            ),
        ],
    )

    await clickhouse_client.store_task_run(run, settings={"async_insert": 1, "wait_for_async_insert": 1})

    await asyncio.sleep(0.2)

    fetched_run = await clickhouse_client.fetch_task_run_resource(_TASK_TUPLE, run.id)
    assert fetched_run is not None
    assert fetched_run.id == run.id
    assert fetched_run.cost_usd == 11
    assert fetched_run.duration_seconds == 12
    assert fetched_run.reasoning_steps == run.reasoning_steps
    assert fetched_run.tool_calls == run.tool_calls
    assert fetched_run.tool_call_requests == run.tool_call_requests
    assert fetched_run.llm_completions == run.llm_completions


class TestStoreTaskRun:
    async def test_store_task_run(self, clickhouse_client: ClickhouseClient):
        # actual_run_stored in out database
        raw = bson.json_util.loads(fixture_bytes("runs/run_doc_1.json"))  # type: ignore
        run = TaskRunDocument.model_validate(raw).to_resource()
        run.task_uid = 1

        # Check that we don't throw
        await clickhouse_client.store_task_run(run)


class TestSearchTaskRun:
    @classmethod
    async def _insert_runs(cls, clickhouse_client: ClickhouseClient, *args: Run):
        docs = [ClickhouseRun.from_domain(1, r) for r in args]
        for i, doc in enumerate(docs):
            doc.run_uuid = _uuid7(i + 1)
            if not doc.task_uid:
                doc.task_uid = 1

        await clickhouse_client.insert_models("runs", docs, {"async_insert": 0, "wait_for_async_insert": 0})
        return docs

    @classmethod
    async def _search(cls, clickhouse_client: ClickhouseClient, query: list[SearchQuery], task_uid: int = 1):
        return sorted(
            [r.id async for r in clickhouse_client.search_task_runs(("", task_uid), query, limit=100, offset=0)],
        )

    async def test_search(self, clickhouse_client: ClickhouseClient):
        await self._insert_runs(
            clickhouse_client,
            task_run_ser(),
            task_run_ser(task_uid=2),
        )

        r = await self._search(clickhouse_client, [])
        assert r == [str(_uuid7(1))]

        r = await self._search(clickhouse_client, [], task_uid=2)
        assert r == [str(_uuid7(2))]

    async def test_count(self, clickhouse_client: ClickhouseClient):
        await self._insert_runs(
            clickhouse_client,
            task_run_ser(),
            task_run_ser(task_uid=2),
        )

        r = await clickhouse_client.count_filtered_task_runs(("", 1), [])
        assert r == 1

        r = await clickhouse_client.count_filtered_task_runs(("", 1), [])
        assert r == 1

    @pytest.mark.parametrize(
        ("eval_hash", "expected"),
        [
            (["f0f1553134f423df626e2d797fdd005e"], [1]),
            ({"f0f1553134f423df626e2d797fdd005e"}, [1]),  # with a set
            (["f0f1553134f423df626e2d797fdd005e", "0732e90b9bb0301d4bec56ef79b5d659"], [1, 2]),  # 2 values
        ],
    )
    async def test_search_reviews(
        self,
        clickhouse_client: ClickhouseClient,
        eval_hash: Collection[str],
        expected: list[int],
    ):
        await self._insert_runs(
            clickhouse_client,
            task_run_ser(),
            task_run_ser(task_input_hash="123"),
        )

        r = await self._search(
            clickhouse_client,
            [
                SearchQuerySimple(SearchField.EVAL_HASH, operation=SearchOperationSingle(SearchOperator.IS, eval_hash)),
            ],
        )
        assert r == [str(_uuid7(i)) for i in expected]

    @pytest.mark.parametrize(
        ("schema_id", "operator", "expected"),
        [
            (1, SearchOperator.IS, [1]),
            (2, SearchOperator.IS_NOT, [1, 3]),
            (2, SearchOperator.LESS_THAN, [1]),
            (2, SearchOperator.GREATER_THAN, [3]),
        ],
    )
    async def test_search_schema_id(
        self,
        clickhouse_client: ClickhouseClient,
        schema_id: int,
        operator: SingleValueOperator,
        expected: list[int],
    ):
        await self._insert_runs(
            clickhouse_client,
            task_run_ser(task_schema_id=1),
            task_run_ser(task_schema_id=2),
            task_run_ser(task_schema_id=3),
        )

        r = await self._search(
            clickhouse_client,
            [SearchQuerySimple(SearchField.SCHEMA_ID, operation=SearchOperationSingle(operator, schema_id))],
        )
        assert r == [str(_uuid7(i)) for i in expected]

    @pytest.mark.parametrize(
        ("version", "operator", "expected"),
        [
            ("abcde", SearchOperator.IS, [1]),
            ("abcdef", SearchOperator.IS_NOT, [1]),
        ],
    )
    async def test_search_version(
        self,
        clickhouse_client: ClickhouseClient,
        version: str,
        operator: SingleValueOperator,
        expected: list[int],
    ):
        await self._insert_runs(
            clickhouse_client,
            task_run_ser(group_kwargs={"id": "abcde"}),
            task_run_ser(group_kwargs={"id": "abcdef"}),
        )

        r = await self._search(
            clickhouse_client,
            [SearchQuerySimple(SearchField.VERSION, operation=SearchOperationSingle(operator, version))],
        )
        assert r == [str(_uuid7(i)) for i in expected]

    @pytest.mark.parametrize(
        ("price", "operator", "expected"),
        [
            (10, SearchOperator.IS, [2]),
            (10, SearchOperator.LESS_THAN, [1]),
            (10, SearchOperator.GREATER_THAN, [3]),
            (10, SearchOperator.GREATER_THAN_OR_EQUAL_TO, [2, 3]),
        ],
    )
    async def test_search_price(
        self,
        clickhouse_client: ClickhouseClient,
        price: int,
        operator: SingleValueOperator,
        expected: list[int],
    ):
        await self._insert_runs(
            clickhouse_client,
            task_run_ser(cost_usd=5),
            task_run_ser(cost_usd=10),
            task_run_ser(cost_usd=20),
        )

        r = await self._search(
            clickhouse_client,
            [SearchQuerySimple(SearchField.PRICE, operation=SearchOperationSingle(operator, price))],
        )
        assert r == [str(_uuid7(i)) for i in expected]

    @pytest.mark.parametrize(
        ("latency", "operator", "expected"),
        [
            (10, SearchOperator.IS, [2]),
            (10, SearchOperator.LESS_THAN, [1]),
            (10, SearchOperator.GREATER_THAN, [3]),
        ],
    )
    async def test_search_latency(
        self,
        clickhouse_client: ClickhouseClient,
        latency: int,
        operator: SingleValueOperator,
        expected: list[int],
    ):
        await self._insert_runs(
            clickhouse_client,
            task_run_ser(duration_seconds=5),
            task_run_ser(duration_seconds=10),
            task_run_ser(duration_seconds=20),
        )

        r = await self._search(
            clickhouse_client,
            [SearchQuerySimple(SearchField.LATENCY, operation=SearchOperationSingle(operator, latency))],
        )
        assert r == [str(_uuid7(i)) for i in expected]

    @pytest.mark.parametrize(
        ("temperature", "operator", "expected"),
        [
            (0.5, SearchOperator.IS, [1]),
            (0.5, SearchOperator.LESS_THAN, [2]),
            (0.5, SearchOperator.GREATER_THAN, [3]),
        ],
    )
    async def test_search_temperature(
        self,
        clickhouse_client: ClickhouseClient,
        temperature: float,
        operator: SingleValueOperator,
        expected: list[int],
    ):
        await self._insert_runs(
            clickhouse_client,
            task_run_ser(group=task_group(properties={"temperature": 0.5})),
            task_run_ser(group=task_group(properties={"temperature": 0.2})),
            task_run_ser(group=task_group(properties={"temperature": 0.8})),
        )

        r = await self._search(
            clickhouse_client,
            [SearchQuerySimple(SearchField.TEMPERATURE, operation=SearchOperationSingle(operator, temperature))],
        )
        assert r == [str(_uuid7(i)) for i in expected]

    @pytest.mark.parametrize(
        ("model", "operator", "expected"),
        [
            ("o1-mini-2024-09-12", SearchOperator.IS, [1]),
            ("o1-mini-2024-09-12", SearchOperator.IS_NOT, [2]),
            ("o1-mini", SearchOperator.CONTAINS, [1, 2]),
            ("13", SearchOperator.NOT_CONTAINS, [1]),
        ],
    )
    async def test_search_model(
        self,
        clickhouse_client: ClickhouseClient,
        model: str,
        operator: SingleValueOperator,
        expected: list[int],
    ):
        await self._insert_runs(
            clickhouse_client,
            task_run_ser(group=task_group(properties={"model": "o1-mini-2024-09-12"})),
            task_run_ser(group=task_group(properties={"model": "o1-mini-2024-09-13"})),
        )

        r = await self._search(
            clickhouse_client,
            [SearchQuerySimple(SearchField.MODEL, operation=SearchOperationSingle(operator, model))],
        )
        assert r == [str(_uuid7(i)) for i in expected]

    @pytest.mark.parametrize(
        ("key", "value", "field_type", "operator", "expected"),
        [
            ("name", "test", "string", SearchOperator.IS, [1]),
            ("name", "test", "string", SearchOperator.IS_NOT, [2, 3, 4]),
            ("name", "T", "string", SearchOperator.CONTAINS, [1, 2]),
            ("name", None, "string", SearchOperator.IS_EMPTY, [3, 4]),
            ("values", 2, "array_length", SearchOperator.IS, [3]),
            ("values", 1, "array_length", SearchOperator.GREATER_THAN, [3, 4]),
            ("values[]", 3, "integer", SearchOperator.IS, [4]),
            ("values[]", 2, "integer", SearchOperator.GREATER_THAN_OR_EQUAL_TO, [3, 4]),
            ("array[].a", "a1", "string", SearchOperator.IS, [1]),
            ("array[].a", "a2", "string", SearchOperator.IS, [1]),
        ],
    )
    async def test_search_input(
        self,
        clickhouse_client: ClickhouseClient,
        key: str,
        value: str,
        field_type: FieldType,
        operator: SingleValueOperator,
        expected: list[int],
    ):
        await self._insert_runs(
            clickhouse_client,
            task_run_ser(task_input={"name": "test", "array": [{"a": "a1"}, {"a": "a2"}]}),
            task_run_ser(task_input={"name": "test1"}),
            task_run_ser(task_input={"values": [1, 2], "nested": {"a": "b"}}),
            task_run_ser(task_input={"values": [1, 2, 3]}),
        )

        r = await self._search(
            clickhouse_client,
            [
                SearchQueryNested(
                    SearchField.INPUT,
                    operation=SearchOperationSingle(operator, value),
                    key_path=key,
                    field_type=field_type,
                ),
            ],
        )
        assert r == [str(_uuid7(i)) for i in expected]

    @pytest.mark.parametrize(
        ("status", "operator", "expected"),
        [
            ("success", SearchOperator.IS, [1]),
            ("failure", SearchOperator.IS, [2]),
        ],
    )
    async def test_search_status(
        self,
        clickhouse_client: ClickhouseClient,
        status: str,
        operator: SingleValueOperator,
        expected: list[int],
    ):
        await self._insert_runs(
            clickhouse_client,
            task_run_ser(status="success"),
            task_run_ser(status="failure", error={"message": "test", "code": "test", "status_code": 500}),
        )

        r = await self._search(
            clickhouse_client,
            [SearchQuerySimple(SearchField.STATUS, operation=SearchOperationSingle(SearchOperator.IS, "success"))],
        )
        assert r == [str(_uuid7(1))]

    @pytest.mark.parametrize(
        ("operation", "expected_indices"),
        [
            # TODO: check why this test fails
            # pytest.param(
            #     SearchOperationBetween(
            #         SearchOperator.IS_BETWEEN,
            #         (datetime.datetime(2024, 1, 1, 12), datetime.datetime(2024, 1, 1, 14)),
            #     ),
            #     [2, 3, 4],  # includes 12:00, 13:00, and 14:00
            #     id="between",
            # ),
            # IS_BEFORE test - excludes boundary
            pytest.param(
                SearchOperationSingle(SearchOperator.IS_BEFORE, datetime.datetime(2024, 1, 1, 12)),
                [0, 1],  # includes only 11:00
                id="before",
            ),
            # IS_AFTER test - excludes boundary
            pytest.param(
                SearchOperationSingle(SearchOperator.IS_AFTER, datetime.datetime(2024, 1, 1, 12, 1)),
                [2, 3],  # includes 13:00 and 14:00
                id="after",
            ),
        ],
    )
    async def test_search_time(
        self,
        clickhouse_client: ClickhouseClient,
        operation: SearchOperation,
        expected_indices: list[int],
    ):
        """Test searching runs by time with different operators."""
        base_time = datetime.datetime(2024, 1, 1, 12)  # noon
        runs = [
            _ck_run(created_at=base_time - datetime.timedelta(hours=1)),  # 11:00
            _ck_run(created_at=base_time),  # 12:00
            _ck_run(created_at=base_time + datetime.timedelta(hours=1)),  # 13:00
            _ck_run(created_at=base_time + datetime.timedelta(hours=2)),  # 14:00
        ]
        await clickhouse_client.insert_models("runs", runs, {"async_insert": 0, "wait_for_async_insert": 0})

        query: list[SearchQuery] = [SearchQuerySimple(SearchField.TIME, operation=operation)]
        r = await self._search(clickhouse_client, query)
        indices = {str(r.run_uuid): idx for idx, r in enumerate(runs)}
        assert sorted([indices[i] for i in r]) == expected_indices


class TestFetchCachedRun:
    async def test_fetch_cached_run(self, clickhouse_client: ClickhouseClient):
        """Test for success only runs"""
        uuid = uuid7()
        run = task_run_ser(
            status="success",
            id=str(uuid),
            task_uid=1,
            task_input={"name": "test"},
        )
        docs = [ClickhouseRun.from_domain(1, run)]
        assert docs[0].cache_hash == "4984014be755b9be7f3cbe9c23fea69a", "sanity"
        await clickhouse_client.insert_models("runs", docs, {"async_insert": 0, "wait_for_async_insert": 0})

        fetched_run = await clickhouse_client.fetch_cached_run(_TASK_TUPLE, 1, run.task_input_hash, run.group.id, None)
        assert fetched_run
        assert fetched_run.id == str(uuid)
        assert fetched_run.llm_completions is None

    async def test_fetch_cached_run_ordered(self, clickhouse_client: ClickhouseClient):
        """Check that the most recent run is returned for cache"""

        # Create an insert a run 1
        now_ms = int(time.time() * 1000)
        uuid1 = uuid7(ms=lambda: now_ms - 1)
        run1 = task_run_ser(id=str(uuid1), task_uid=1, task_input={"name": "test"}, task_output={"output": 1})

        await clickhouse_client.insert_models(
            "runs",
            [ClickhouseRun.from_domain(1, run1)],
            {"async_insert": 0, "wait_for_async_insert": 0},
        )
        fetched_run = await clickhouse_client.fetch_cached_run(
            _TASK_TUPLE,
            1,
            run1.task_input_hash,
            run1.group.id,
            None,
        )
        assert fetched_run
        assert fetched_run.id == str(uuid1)

        # Create and insert a run 2 that has the same input hash but was created 1ms later than run 1
        uuid2 = uuid7(ms=lambda: now_ms)
        assert uuid2 > uuid1, "sanity"

        run2 = task_run_ser(id=str(uuid2), task_uid=1, task_input={"name": "test"}, task_output={"output": 2})
        await clickhouse_client.insert_models(
            "runs",
            [ClickhouseRun.from_domain(1, run2)],
            {"async_insert": 0, "wait_for_async_insert": 0},
        )

        fetched_run = await clickhouse_client.fetch_cached_run(
            _TASK_TUPLE,
            1,
            run1.task_input_hash,
            run1.group.id,
            None,
        )
        assert fetched_run
        assert fetched_run.id == str(uuid2)

    async def test_fetch_cached_run_fail(self, clickhouse_client: ClickhouseClient):
        """Test when we are not fetching success only runs"""
        uuid = uuid7()
        run = task_run_ser(
            id=str(uuid),
            task_uid=1,
            task_input={"name": "test"},
            status="failure",
            error=ErrorResponse.Error(message="test", code="test", status_code=500),
        )

        docs = [ClickhouseRun.from_domain(1, run)]
        assert docs[0].cache_hash == "4984014be755b9be7f3cbe9c23fea69a", "sanity"
        await clickhouse_client.insert_models("runs", docs, {"async_insert": 0, "wait_for_async_insert": 0})

        fetched_run = await clickhouse_client.fetch_cached_run(_TASK_TUPLE, 1, run.task_input_hash, run.group.id, None)
        assert fetched_run is None

        fetched_run = await clickhouse_client.fetch_cached_run(
            _TASK_TUPLE,
            1,
            run.task_input_hash,
            run.group.id,
            None,
            success_only=False,
        )
        assert fetched_run is not None

    async def test_fetch_cached_run_success_only_no_output(self, clickhouse_client: ClickhouseClient):
        """Successful runs can have no output in case of tool calls for example. This tests ensure
        that empty outputs are not returned when success only is requested.
        This tests also checks"""
        now_ms = int(time.time() * 1000)
        uuid1 = uuid7(ms=lambda: now_ms - 1)
        run1 = task_run_ser(id=str(uuid1), task_uid=1, task_input={"name": "test"}, task_output={"output": 1})
        uuid2 = uuid7(ms=lambda: now_ms)
        run2 = task_run_ser(id=str(uuid2), task_uid=1, task_input={"name": "test"}, task_output={})

        await clickhouse_client.insert_models(
            "runs",
            [ClickhouseRun.from_domain(1, run1), ClickhouseRun.from_domain(1, run2)],
            {"async_insert": 0, "wait_for_async_insert": 0},
        )

        # Sanity check, fetch the first run with success only = False to make sure we get the latest run
        fetched_run = await clickhouse_client.fetch_cached_run(
            _TASK_TUPLE,
            1,
            run1.task_input_hash,
            run1.group.id,
            None,
            success_only=False,
        )
        assert fetched_run
        assert fetched_run.id == str(uuid2)

        # Now fetch the run with success only = True to make sure we get the first run
        fetched_run = await clickhouse_client.fetch_cached_run(
            _TASK_TUPLE,
            1,
            run1.task_input_hash,
            run1.group.id,
            None,
            success_only=True,
        )
        assert fetched_run
        assert fetched_run.id == str(uuid1)


def _ck_run(task_uid: int = 1, tenant_uid: int = 1, created_at: datetime.datetime | None = None, **kwargs: Any):
    uuid = uuid7() if created_at is None else uuid7(ms=lambda: int(created_at.timestamp() * 1000))
    run = task_run_ser(
        id=str(uuid),
        task_uid=task_uid,
        created_at=created_at or datetime_factory(),
        **kwargs,
    )
    return ClickhouseRun.from_domain(tenant_uid, run)


class TestAggregateTaskMetadataFields:
    async def test_aggregate_task_metadata_fields(self, clickhouse_client: ClickhouseClient):
        models = [
            _ck_run(metadata={"a": "b"}),
            _ck_run(metadata={"a": "1", "c": "d"}),
            _ck_run(metadata={"a": "2", "c": "d", "e": "f"}),
            _ck_run(metadata={"a": "3", "c": "d", "e": "f", "g": "h"}),
            _ck_run(task_uid=2, metadata={"r": "s"}),
            _ck_run(tenant_uid=2, metadata={"s": "s"}),
        ]
        await clickhouse_client.insert_models("runs", models, {"async_insert": 0, "wait_for_async_insert": 0})

        fields = {k: sorted(v) async for k, v in clickhouse_client.aggregate_task_metadata_fields(("", 1))}
        assert fields == {"a": ["1", "2", "3", "b"], "c": ["d"], "e": ["f"], "g": ["h"]}

    async def test_exclude_prefix(self, clickhouse_client: ClickhouseClient):
        models = [
            _ck_run(metadata={"a": "b"}),
            _ck_run(metadata={"a": "1", "c": "d"}),
            _ck_run(metadata={"a": "2", "c": "d", "e": "f"}),
            _ck_run(metadata={"workflowai.a": "b"}),
            _ck_run(metadata={"workflowai.a": "1", "c": "d"}),
            _ck_run(metadata={"workflowai.a": "2", "c": "d", "e": "f"}),
        ]
        await clickhouse_client.insert_models("runs", models, {"async_insert": 0, "wait_for_async_insert": 0})

        fields = {
            k: sorted(v) async for k, v in clickhouse_client.aggregate_task_metadata_fields(("", 1), "workflowai.")
        }
        assert fields == {"a": ["1", "2", "b"], "c": ["d"], "e": ["f"]}


def _qs(task_uid: int = 1, exp: list[int] = [1, 2, 3, 4, 5], **kwargs: Any):
    # TODO: it would be nice to return pytest.param here but it creates discovery errors where computing a simple id
    return (
        task_uid,
        exp,
        SerializableTaskRunQuery(task_id="", **kwargs),
    )


class TestFetchTaskRunResources:
    @pytest.mark.parametrize(
        ("task_uid", "expected", "query"),
        [
            _qs(exp=[0, 1, 2, 3, 4]),
            _qs(exp=[0, 2, 3, 4], status={"success"}),
            _qs(exp=[1], task_input_hashes={"10e98b24438459b86a3ddd72b495cfb3"}),
            # TODO: we should add a real test for unique by, the test data does not include duplicate values
            _qs(exp=[1], task_input_hashes={"10e98b24438459b86a3ddd72b495cfb3"}, unique_by={"task_input_hash"}),
            _qs(
                exp=[1],
                task_input_hashes={"10e98b24438459b86a3ddd72b495cfb3"},
                unique_by={"task_input_hash", "task_output_hash"},
            ),
            _qs(
                exp=[1, 2],
                task_input_hashes={"10e98b24438459b86a3ddd72b495cfb3", "c0629dbc56e3fbaa6c0620155270966f"},
            ),
            _qs(
                exp=[1],
                task_schema_id=2,
                task_input_hashes={"10e98b24438459b86a3ddd72b495cfb3"},
                task_output_hashes={"0a5a3c773e3f8def55c94f90ad1b4d7b"},
            ),
            _qs(exp=[1, 2, 4], metadata={"provider": "openai"}),
        ],
    )
    async def test_fetch_task_run_resources(
        self,
        clickhouse_client: ClickhouseClient,
        task_uid: int,
        expected: list[int],
        query: SerializableTaskRunQuery,
    ):
        docs = fixtures_json("runs/searchable_runs.json")
        docs = [ClickhouseRun.model_validate(d) for d in docs]
        await clickhouse_client.insert_models("runs", docs, {"async_insert": 0, "wait_for_async_insert": 0})

        doc_map = {str(d.run_uuid): idx for idx, d in enumerate(docs)}
        r = [doc_map[r.id] async for r in clickhouse_client.fetch_task_run_resources(task_uid, query)]
        assert r == expected

    async def test_unique_version_ids(self, clickhouse_client: ClickhouseClient):
        hash_arg: dict[str, Any] = {
            "task_input_hash": "input1",
            "task_output_hash": "output1",
        }
        await clickhouse_client.insert_models(
            "runs",
            [
                _ck_run(task_schema_id=3, cost_usd=102.0),
                _ck_run(**hash_arg),
                _ck_run(group_id="v2", **hash_arg),
                _ck_run(group_id="v3", **hash_arg),
                # Not included because the input hash is different
                _ck_run(group_id="v4", task_input_hash="1", task_output_hash="output1"),
                # Not included because the output hash is different
                _ck_run(group_id="v5", task_input_hash="input1", task_output_hash="1"),
            ],
            {"async_insert": 0, "wait_for_async_insert": 0},
        )

        q = SerializableTaskRunQuery(
            task_id="task_id",
            task_schema_id=1,
            task_input_hashes={"input1"},
            task_output_hash="output1",
            unique_by={"version_id"},
            include_fields={"version_id"},
        )
        version_ids = {run.group.id async for run in clickhouse_client.fetch_task_run_resources(1, q)}
        assert version_ids == {"group_alias", "v2", "v3"}

        q.group_ids = {"v2", "v3"}

        version_ids = {run.group.id async for run in clickhouse_client.fetch_task_run_resources(1, q)}
        assert version_ids == {"v2", "v3"}


def _llm_completion(prompt_token_count: int, completion_token_count: int):
    return LLMCompletion(
        messages=[],
        usage=LLMUsage(prompt_token_count=prompt_token_count, completion_token_count=completion_token_count),
        provider=Provider.OPEN_AI,
    )


class TestAggregateTokenCounts:
    async def test_aggregate_token_counts(self, clickhouse_client: ClickhouseClient):
        await clickhouse_client.insert_models(
            "runs",
            [
                _ck_run(llm_completions=[_llm_completion(12, 20)]),
                _ck_run(llm_completions=[_llm_completion(15, 25)]),
                _ck_run(llm_completions=[_llm_completion(9, 15)]),
            ],
            {"async_insert": 0, "wait_for_async_insert": 0},
        )
        result = await clickhouse_client.aggregate_token_counts(("", 1), 1)
        expected_prompt_avg = (12 + 15 + 9) / 3
        expected_completion_avg = (20 + 25 + 15) / 3
        assert result["average_prompt_tokens"] == expected_prompt_avg
        assert result["average_completion_tokens"] == expected_completion_avg

    async def test_aggregate_token_counts_excluded_models(self, clickhouse_client: ClickhouseClient):
        task_runs = [
            _ck_run(llm_completions=[_llm_completion(12, 20)]),
            _ck_run(llm_completions=[_llm_completion(13, 200)], model="o1-preview-2024-09-12"),
            _ck_run(llm_completions=[_llm_completion(14, 201)], model="o1-mini-2024-09-12"),
        ]
        await clickhouse_client.insert_models("runs", task_runs, {"async_insert": 0, "wait_for_async_insert": 0})

        result = await clickhouse_client.aggregate_token_counts(
            ("", 1),
            1,
            excluded_models=["o1-preview-2024-09-12", "o1-mini-2024-09-12"],
        )

        assert result["average_prompt_tokens"] == 12
        assert result["average_completion_tokens"] == 20

    async def test_aggregate_token_counts_included_models(self, clickhouse_client: ClickhouseClient):
        task_runs = [
            _ck_run(llm_completions=[_llm_completion(12, 20)]),
            _ck_run(llm_completions=[_llm_completion(13, 200)], model="o1-preview-2024-09-12"),
            _ck_run(llm_completions=[_llm_completion(14, 201)], model="o1-mini-2024-09-12"),
        ]
        await clickhouse_client.insert_models("runs", task_runs, {"async_insert": 0, "wait_for_async_insert": 0})

        result = await clickhouse_client.aggregate_token_counts(
            ("", 1),
            1,
            included_models=["o1-preview-2024-09-12", "o1-mini-2024-09-12"],
        )
        assert result["average_prompt_tokens"] == (13 + 14) / 2
        assert result["average_completion_tokens"] == (200 + 201) / 2


class TestAggregateTaskRunCosts:
    async def test_aggregate_task_run_costs(self, clickhouse_client: ClickhouseClient):
        await clickhouse_client.insert_models(
            "runs",
            [
                _ck_run(cost_usd=100, created_at=datetime.datetime(2024, 1, 1)),
                _ck_run(cost_usd=200, created_at=datetime.datetime(2024, 1, 2)),
                _ck_run(cost_usd=300, created_at=datetime.datetime(2024, 1, 3)),
            ],
            {"async_insert": 0, "wait_for_async_insert": 0},
        )
        result = [
            r async for r in clickhouse_client.aggregate_task_run_costs(1, SerializableTaskRunQuery(task_id="task_id"))
        ]
        assert result == [
            TaskRunAggregatePerDay(date=datetime.date(2024, 1, 1), total_count=1, total_cost_usd=100),
            TaskRunAggregatePerDay(date=datetime.date(2024, 1, 2), total_count=1, total_cost_usd=200),
            TaskRunAggregatePerDay(date=datetime.date(2024, 1, 3), total_count=1, total_cost_usd=300),
        ]


class TestAggregateRuns:
    # TODO[clickhouse]: add a test with filters
    async def test_aggregate_runs(self, clickhouse_client: ClickhouseClient):
        await clickhouse_client.insert_models(
            "runs",
            [
                # Another tenant, should be ignored
                _ck_run(tenant_uid=2, cost_usd=110.75),
                # Another task schema, should be ignored
                _ck_run(task_schema_id=3, cost_usd=102.0),
                _ck_run(
                    cost_usd=1.0,
                    task_input_hash="i1",
                    task_output_hash="o1",
                    duration_seconds=1,
                ),
                # Insert a run for a duplicate input & output, it should be ignored
                _ck_run(
                    cost_usd=1.0,
                    task_input_hash="i1",
                    task_output_hash="o1",
                    duration_seconds=1,
                ),
                # Insert a run for a duplicate input, it should NOT be ignored
                _ck_run(cost_usd=2.0, task_input_hash="i1"),
                # User review should prime,
                _ck_run(cost_usd=6.0, error={"message": "test"}, status="failure"),
                # no review
                _ck_run(cost_usd=1.0),
                _ck_run(group_id="2", cost_usd=1, duration_seconds=2),
            ],
            {"async_insert": 0, "wait_for_async_insert": 0},
        )

        agg = await clickhouse_client.aggregate_runs(("", 1), 1, set(), None)
        for v in agg.values():
            v["eval_hashes"] = sorted(v["eval_hashes"])

        assert agg == {
            "group_alias": RunAggregate(
                average_cost_usd=pytest.approx(2.2, abs=0.01),  # type: ignore
                average_duration_seconds=1.0,
                total_run_count=5,
                failed_run_count=1,
                eval_hashes=[
                    "8d0705a43526930ebcce6a561e63efc8",
                    "f0f1553134f423df626e2d797fdd005e",
                    "f0f1553134f423df626e2d797fdd005e",
                    "f0fb628f228fe1bbf00db390ff350f7b",
                    "f0fb628f228fe1bbf00db390ff350f7b",
                ],
            ),
            "2": RunAggregate(
                average_cost_usd=1.0,
                average_duration_seconds=2.0,
                total_run_count=1,
                failed_run_count=0,
                eval_hashes=["f0f1553134f423df626e2d797fdd005e"],
            ),
        }

    async def test_duplicate_input_output(self, clickhouse_client: ClickhouseClient):
        runs = [
            _ck_run(task_input_hash="i1", task_output_hash="o1", group_id="1"),
            _ck_run(task_input_hash="i1", task_output_hash="o1", group_id="2"),
        ]
        await clickhouse_client.insert_models("runs", runs, {"async_insert": 0, "wait_for_async_insert": 0})

        agg = await clickhouse_client.aggregate_runs(("", 1), 1, set(), None)

        assert agg == {
            "1": RunAggregate(
                average_cost_usd=0.0,
                average_duration_seconds=1.0,
                total_run_count=1,
                failed_run_count=0,
                eval_hashes=["f0fb628f228fe1bbf00db390ff350f7b"],
            ),
            "2": RunAggregate(
                average_cost_usd=0.0,
                average_duration_seconds=1.0,
                total_run_count=1,
                failed_run_count=0,
                eval_hashes=["f0fb628f228fe1bbf00db390ff350f7b"],
            ),
        }

    async def test_aggregate_runs_with_filters(self, clickhouse_client: ClickhouseClient):
        # Create test data with different input hashes and group IDs
        runs = [
            _ck_run(task_input_hash="i1", task_output_hash="o1", group_id="v1", cost_usd=1.0, duration_seconds=10),
            _ck_run(task_input_hash="i2", task_output_hash="o2", group_id="v1", cost_usd=2.0, duration_seconds=20),
            _ck_run(task_input_hash="i3", task_output_hash="o3", group_id="v2", cost_usd=3.0, duration_seconds=30),
            _ck_run(task_input_hash="i4", task_output_hash="o4", group_id="v3", cost_usd=4.0, duration_seconds=40),
        ]
        await clickhouse_client.insert_models("runs", runs, {"async_insert": 0, "wait_for_async_insert": 0})

        # Test filtering by input hashes
        agg = await clickhouse_client.aggregate_runs(("", 1), 1, {"i1", "i2"}, None)
        for v in agg.values():
            v["eval_hashes"] = sorted(v["eval_hashes"])

        assert agg == {
            "v1": RunAggregate(
                average_cost_usd=1.5,
                average_duration_seconds=15.0,
                total_run_count=2,
                failed_run_count=0,
                eval_hashes=["25a840ee59d16a897cd3c27bd55d1db5", "f0fb628f228fe1bbf00db390ff350f7b"],
            ),
        }

        # Test filtering by group IDs
        agg = await clickhouse_client.aggregate_runs(("", 1), 1, set(), {"v2", "v3"})
        for v in agg.values():
            v["eval_hashes"] = sorted(v["eval_hashes"])

        assert agg == {
            "v2": RunAggregate(
                average_cost_usd=3.0,
                average_duration_seconds=30.0,
                total_run_count=1,
                failed_run_count=0,
                eval_hashes=["31d8859c3f9d4e9a4bd2dc163bb2c3d6"],
            ),
            "v3": RunAggregate(
                average_cost_usd=4.0,
                average_duration_seconds=40.0,
                total_run_count=1,
                failed_run_count=0,
                eval_hashes=["31bea3e3766fb0f2b5f8d13bca6ab627"],
            ),
        }

        # Test filtering by both input hashes and group IDs
        agg = await clickhouse_client.aggregate_runs(("", 1), 1, {"i1", "i3"}, {"v1", "v2"})
        for v in agg.values():
            v["eval_hashes"] = sorted(v["eval_hashes"])

        assert agg == {
            "v1": RunAggregate(
                average_cost_usd=1.0,
                average_duration_seconds=10.0,
                total_run_count=1,
                failed_run_count=0,
                eval_hashes=["f0fb628f228fe1bbf00db390ff350f7b"],
            ),
            "v2": RunAggregate(
                average_cost_usd=3.0,
                average_duration_seconds=30.0,
                total_run_count=1,
                failed_run_count=0,
                eval_hashes=["31d8859c3f9d4e9a4bd2dc163bb2c3d6"],
            ),
        }


class TestFetchRun:
    async def test_include_status(self, clickhouse_client: ClickhouseClient):
        run = _ck_run(status="success")
        await clickhouse_client.insert_models(
            "runs",
            [run],
            {"async_insert": 0, "wait_for_async_insert": 0},
        )
        run = await clickhouse_client.fetch_task_run_resource(("bla", 1), str(run.run_uuid), include={"status"})
        assert run.status == "success"
        assert run.task_id == "bla"

    async def test_include_version_iteration(self, clickhouse_client: ClickhouseClient):
        run = _ck_run(group_id="1", version_iteration=1)
        await clickhouse_client.insert_models(
            "runs",
            [run],
            {"async_insert": 0, "wait_for_async_insert": 0},
        )
        run = await clickhouse_client.fetch_task_run_resource(
            ("bla", 1),
            str(run.run_uuid),
            include={"group.iteration"},
        )
        assert run.group.iteration == 1


class TestRunCountByVersionId:
    async def test_run_count_by_version_id(self, clickhouse_client: ClickhouseClient):
        # Create test data with different version IDs and dates
        now = datetime.datetime(2024, 1, 1)
        runs = [
            # These should be counted (same agent, after from_date)
            _ck_run(task_uid=1, group_id="v1", created_at=now),
            _ck_run(task_uid=1, group_id="v1", created_at=now),
            _ck_run(task_uid=1, group_id="v2", created_at=now),
            # Different agent, should not be counted
            _ck_run(task_uid=2, group_id="v1", created_at=now),
            # Before from_date, should not be counted
            _ck_run(task_uid=1, group_id="v1", created_at=now - datetime.timedelta(days=2)),
            # Different tenant, should not be counted
            _ck_run(tenant_uid=2, task_uid=1, group_id="v1", created_at=now),
        ]
        await clickhouse_client.insert_models("runs", runs, {"async_insert": 0, "wait_for_async_insert": 0})

        counts = {
            r.version_id: r.run_count
            async for r in clickhouse_client.run_count_by_version_id(1, now - datetime.timedelta(days=1))
        }
        assert counts == {"v1": 2, "v2": 1}

    async def test_run_count_by_version_id_empty(self, clickhouse_client: ClickhouseClient):
        # Test with no data
        now = datetime.datetime(2024, 1, 1)
        counts = {r.version_id: r.run_count async for r in clickhouse_client.run_count_by_version_id(1, now)}
        assert counts == {}

    async def test_run_count_by_version_id_date_boundary(self, clickhouse_client: ClickhouseClient):
        # Test exact date boundary behavior
        boundary_date = datetime.datetime(2024, 1, 1, 12, 0)  # Noon on Jan 1st
        runs = [
            # 1 second before boundary
            _ck_run(task_uid=1, group_id="v1", created_at=boundary_date - datetime.timedelta(seconds=1)),
            # Exactly at boundary
            _ck_run(task_uid=1, group_id="v1", created_at=boundary_date),
            # 1 second after boundary
            _ck_run(task_uid=1, group_id="v1", created_at=boundary_date + datetime.timedelta(seconds=1)),
        ]
        await clickhouse_client.insert_models("runs", runs, {"async_insert": 0, "wait_for_async_insert": 0})

        # Should only include runs at or after boundary_date
        counts = {r.version_id: r.run_count async for r in clickhouse_client.run_count_by_version_id(1, boundary_date)}
        assert counts == {"v1": 2}


class TestRunCountByAgentUid:
    async def test_run_count_by_agent_uid(self, clickhouse_client: ClickhouseClient):
        # Create test data with different agent UIDs and dates
        now = datetime.datetime(2024, 1, 1)
        runs = [
            # These should be counted (after from_date)
            _ck_run(task_uid=1, created_at=now, cost_usd=10.0),
            _ck_run(task_uid=1, created_at=now, cost_usd=20.0),  # avg cost for task_uid 1: 15.0
            _ck_run(task_uid=2, created_at=now, cost_usd=30.0),
            _ck_run(task_uid=2, created_at=now, cost_usd=40.0),  # avg cost for task_uid 2: 35.0
            _ck_run(task_uid=3, created_at=now, cost_usd=50.0),  # avg cost for task_uid 3: 50.0
            # Before from_date, should not be counted
            _ck_run(task_uid=1, created_at=now - datetime.timedelta(days=2), cost_usd=100.0),
            _ck_run(task_uid=2, created_at=now - datetime.timedelta(days=2), cost_usd=200.0),
            # Different tenant, should not be counted
            _ck_run(tenant_uid=2, task_uid=1, created_at=now, cost_usd=300.0),
            _ck_run(tenant_uid=2, task_uid=2, created_at=now, cost_usd=400.0),
        ]
        await clickhouse_client.insert_models("runs", runs, {"async_insert": 0, "wait_for_async_insert": 0})

        results = {
            r.agent_uid: (r.run_count, r.total_cost_usd)
            async for r in clickhouse_client.run_count_by_agent_uid(now - datetime.timedelta(days=1))
        }
        assert results == {
            1: (2, pytest.approx(30.0)),  # pyright: ignore [reportUnknownMemberType]
            2: (2, pytest.approx(70.0)),  # pyright: ignore [reportUnknownMemberType]
            3: (1, pytest.approx(50.0)),  # pyright: ignore [reportUnknownMemberType]
        }

    async def test_run_count_by_agent_uid_empty(self, clickhouse_client: ClickhouseClient):
        # Test with no data
        now = datetime.datetime(2024, 1, 1)
        results = {
            r.agent_uid: (r.run_count, r.total_cost_usd) async for r in clickhouse_client.run_count_by_agent_uid(now)
        }
        assert results == {}

    async def test_run_count_by_agent_uid_date_boundary(self, clickhouse_client: ClickhouseClient):
        # Test exact date boundary behavior
        boundary_date = datetime.datetime(2024, 1, 1, 12, 0)  # Noon on Jan 1st
        runs = [
            # 1 second before boundary
            _ck_run(task_uid=1, created_at=boundary_date - datetime.timedelta(seconds=1), cost_usd=10.0),
            # Exactly at boundary
            _ck_run(task_uid=1, created_at=boundary_date, cost_usd=20.0),
            _ck_run(task_uid=2, created_at=boundary_date, cost_usd=30.0),
            # 1 second after boundary
            _ck_run(task_uid=1, created_at=boundary_date + datetime.timedelta(seconds=1), cost_usd=40.0),
            _ck_run(task_uid=2, created_at=boundary_date + datetime.timedelta(seconds=1), cost_usd=50.0),
        ]
        await clickhouse_client.insert_models("runs", runs, {"async_insert": 0, "wait_for_async_insert": 0})

        # Should only include runs at or after boundary_date
        results = {
            r.agent_uid: (r.run_count, r.total_cost_usd)
            async for r in clickhouse_client.run_count_by_agent_uid(boundary_date)
        }
        assert results == {
            1: (2, pytest.approx(60.0)),  # pyright: ignore [reportUnknownMemberType]
            2: (2, pytest.approx(80.0)),  # pyright: ignore [reportUnknownMemberType]
        }
